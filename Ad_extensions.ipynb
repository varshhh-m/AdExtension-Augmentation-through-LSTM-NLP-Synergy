{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk import FreqDist, ngrams\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, LeaveOneOut\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npB4Ov3ZcsQH",
        "outputId": "2671a95a-1ce3-42df-cb81-bd77d6f209cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main(url):\n",
        "  from nltk.corpus import stopwords\n",
        "  import numpy as np\n",
        "  def extract_information(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        html_content = response.text\n",
        "    else:\n",
        "        print(\"Failed to retrieve the webpage:\", response.status_code)\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "\n",
        "    # Extract Title\n",
        "    title = soup.title.string if soup.title else \"\"\n",
        "\n",
        "    # Extract Meta Description\n",
        "    meta_description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
        "    meta_description = meta_description[\"content\"] if meta_description else \"\"\n",
        "\n",
        "    # Extract Header Tags\n",
        "    header_tags = [header.text.strip() for header in soup.find_all([\"h1\", \"h2\", \"h3\"])]\n",
        "\n",
        "    # Extract Text Content\n",
        "    text_content = soup.get_text(strip=True)\n",
        "\n",
        "    # Extract Images\n",
        "    images = [image['src'] for image in soup.find_all('img')]\n",
        "\n",
        "    # Extract Links\n",
        "    links = [link['href'] for link in soup.find_all('a', href=True)]\n",
        "\n",
        "    # Extract Contact Information\n",
        "    contact_info = re.findall(r'(\\+\\d{1,3}\\s)?\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}', html_content)\n",
        "    contact_info = [re.sub(r'[^\\d-]', '', info) for info in contact_info]\n",
        "\n",
        "    # Create a dictionary to store the extracted information\n",
        "    data = {\n",
        "        \"url\": url,\n",
        "        \"title\": title,\n",
        "        \"meta_description\": meta_description,\n",
        "        \"header_tags\": header_tags,\n",
        "        \"text_content\": text_content,\n",
        "        \"images\": images,\n",
        "        \"links\": links,\n",
        "        \"contact_info\": contact_info\n",
        "    }\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "  def save_to_json(data, output_file):\n",
        "      with open(output_file, \"w\") as json_file:\n",
        "          json.dump(data, json_file, indent=4)\n",
        "      #print(\"Data saved to\", output_file)\n",
        "\n",
        "\n",
        "  # Example usage\n",
        "  #url = input(\"Enter the Url\").strip()\n",
        "  output_file = \"website_data.json\"\n",
        "\n",
        "  extracted_data = extract_information(url)\n",
        "  if extracted_data:\n",
        "      save_to_json(extracted_data, output_file)\n",
        "\n",
        "    # Define the path to the JSON file\n",
        "  json_file = \"website_data.json\"\n",
        "\n",
        "  # Load the JSON data\n",
        "  with open(json_file) as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "\n",
        "\n",
        "    # Load the JSON data\n",
        "  json_file = \"website_data.json\"\n",
        "  with open(json_file) as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "  # Preprocessing\n",
        "  def clean_text(text):\n",
        "      # Remove HTML tags\n",
        "      cleaned_text = re.sub('<[^<]+?>', '', text)\n",
        "      # Remove special characters and numbers\n",
        "      cleaned_text = re.sub('[^a-zA-Z]', ' ', cleaned_text)\n",
        "      # Convert to lowercase\n",
        "      cleaned_text = cleaned_text.lower()\n",
        "      # Tokenize the text\n",
        "      tokens = word_tokenize(cleaned_text)\n",
        "      # Remove stopwords\n",
        "      from nltk.corpus import stopwords\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      tokens = [word for word in tokens if word not in stop_words]\n",
        "      # Lemmatize the tokens\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "      tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "      # Join the tokens back to a single string\n",
        "      cleaned_text = ' '.join(tokens)\n",
        "      return cleaned_text\n",
        "\n",
        "  # Apply data preprocessing\n",
        "  cleaned_content = clean_text(data['text_content'])\n",
        "  cleaned_title = clean_text(data['title'])\n",
        "  cleaned_meta_description = clean_text(data['meta_description'])\n",
        "  # NLP Analysis\n",
        "\n",
        "\n",
        "  # Tokenization\n",
        "  tokens = word_tokenize(cleaned_content)\n",
        "  #print(\"Tokens:\", tokens)\n",
        "\n",
        "  # Sentence Tokenization\n",
        "  sentences = sent_tokenize(cleaned_content)\n",
        "  #print(\"Sentences:\", sentences)\n",
        "\n",
        "  # Part-of-Speech (POS) Tagging\n",
        "  pos_tags = nltk.pos_tag(tokens)\n",
        "  #print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "  # Named Entity Recognition (NER)\n",
        "  nltk.download('maxent_ne_chunker')\n",
        "  nltk.download('words')\n",
        "  ner_tags = nltk.ne_chunk(pos_tags)\n",
        "  #print(\"NER Tags:\", ner_tags)\n",
        "\n",
        "  # Sentiment Analysis (example using VaderSentiment)\n",
        "  from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "  analyzer = SentimentIntensityAnalyzer()\n",
        "  sentiment_scores = analyzer.polarity_scores(cleaned_content)\n",
        "  #print(\"Sentiment Scores:\", sentiment_scores)\n",
        "\n",
        "    # Define the number of words in a phrase\n",
        "  phrase_length = 2\n",
        "\n",
        "  # Define stopwords to be ignored during phrase extraction\n",
        "  stopwords = set(stopwords.words('english'))\n",
        "\n",
        "  # Extract keywords\n",
        "  keywords = word_tokenize(cleaned_content)\n",
        "  keyword_freq = FreqDist(keywords)\n",
        "  top_keywords = keyword_freq.most_common(10)  # Extract top 10 most frequent keywords\n",
        "\n",
        "  # Extract important phrases\n",
        "  phrase_freq = FreqDist()\n",
        "  phrases = ngrams(tokens, phrase_length)\n",
        "  for phrase in phrases:\n",
        "      if all(word not in stopwords for word in phrase):\n",
        "          phrase_freq[tuple(phrase)] += 1\n",
        "\n",
        "  top_phrases = phrase_freq.most_common(10)  # Extract top 10 important phrases\n",
        "\n",
        "  # Extract named entities (remaining code remains the same)\n",
        "  entities = [entity for entity in ner_tags if hasattr(entity, 'label')]\n",
        "  named_entities = [ne[0] for entity in entities for ne in entity.leaves()]\n",
        "\n",
        "  # Create a dictionary to store the extracted features\n",
        "  extracted_features = {\n",
        "      \"Top Keywords\": top_keywords,\n",
        "      \"Top Phrases\": top_phrases,\n",
        "      \"Named Entities\": named_entities\n",
        "  }\n",
        "\n",
        "  # Specify the file path\n",
        "  output_file_path = \"output.json\"\n",
        "\n",
        "  # Save the dictionary as JSON\n",
        "  with open(output_file_path, \"w\") as f:\n",
        "      json.dump(extracted_features, f)\n",
        "\n",
        "  #print(\"Output data saved to:\", output_file_path)\n",
        "\n",
        "    # Define the path to the JSON file\n",
        "  json_file = \"output.json\"\n",
        "\n",
        "  # Load the JSON data\n",
        "  with open(json_file) as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "  # Print the loaded data\n",
        "  #print(data)\n",
        "\n",
        "    # Read data from JSON file\n",
        "  with open('output.json', 'r') as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "  # Extract keywords and phrases from the data\n",
        "  keywords = data['Top Keywords']\n",
        "  phrases = data['Top Phrases']\n",
        "\n",
        "  # Flatten the list of keywords and phrases\n",
        "  keywords = [word for sublist in keywords for word in sublist]\n",
        "  phrases = [phrase for sublist in phrases for phrase in sublist]\n",
        "\n",
        "  # Convert the keywords and phrases to string type\n",
        "  keywords = [str(word) for word in keywords]\n",
        "  phrases = [str(phrase) for phrase in phrases]\n",
        "\n",
        "  # Combine keywords and phrases\n",
        "  combined_data = keywords + phrases\n",
        "\n",
        "  # Create a TF-IDF vectorizer\n",
        "  vectorizer = TfidfVectorizer()\n",
        "\n",
        "  # Fit and transform the combined data to obtain the TF-IDF representation\n",
        "  tfidf_matrix = vectorizer.fit_transform(combined_data)\n",
        "\n",
        "  # Get the feature names (keywords) from the vectorizer\n",
        "  feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "  # List to store documents with keywords\n",
        "  documents_with_keywords = []\n",
        "\n",
        "  # Iterate over the documents\n",
        "  for i, doc in enumerate(combined_data):\n",
        "      feature_index = tfidf_matrix[i, :].nonzero()[1]\n",
        "      if len(feature_index) > 0:\n",
        "          # Document contains keywords, add it to the list\n",
        "          documents_with_keywords.append(doc)\n",
        "\n",
        "  # Update combined_data with the filtered documents\n",
        "  combined_data = documents_with_keywords\n",
        "\n",
        "  # Update tfidf_matrix with the filtered documents\n",
        "  tfidf_matrix = vectorizer.transform(combined_data)\n",
        "\n",
        "  # Get the feature names (keywords) from the vectorizer\n",
        "  feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "  # List to store the top keywords for each document\n",
        "  top_keywords_per_document = []\n",
        "\n",
        "  # Print the top keywords with highest TF-IDF scores\n",
        "  num_keywords = 10  # Number of top keywords to extract\n",
        "  for i, doc in enumerate(combined_data):\n",
        "      feature_index = tfidf_matrix[i, :].nonzero()[1]\n",
        "      tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
        "      sorted_tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
        "      top_keywords = [feature_names[i] for i, _ in sorted_tfidf_scores[:num_keywords]]\n",
        "      top_keywords_per_document.append(top_keywords)\n",
        "\n",
        "  # Save the output to a JSON file\n",
        "  output = {'Top Keywords per Document': top_keywords_per_document}\n",
        "\n",
        "  with open('keyword_extraction_output.json', 'w') as file:\n",
        "      json.dump(output, file)\n",
        "\n",
        "\n",
        "    # Specify the path to your JSON file\n",
        "  json_file_path = 'keyword_extraction_output.json'  # Replace with the actual path to your JSON file\n",
        "\n",
        "  # Read the JSON file\n",
        "  with open(json_file_path, 'r') as file:\n",
        "      json_data = json.load(file)\n",
        "\n",
        "  json_data['ad_extensions'] = [\n",
        "      {\n",
        "        \"extension_id\": 1,\n",
        "        \"extension_text\": \"Discover the best deals at our website!\",\n",
        "        \"category\":\"site extensions\"\n",
        "      },\n",
        "      {\n",
        "        \"extension_id\": 2,\n",
        "        \"extension_text\": \"Upgrade your experience at our place\",\n",
        "        \"category\":\"location extension\"\n",
        "      },\n",
        "      {\n",
        "        \"extension_id\": 3,\n",
        "        \"extension_text\": \"Experience our latest collection. call now!\",\n",
        "        \"category\":\"call extension\"\n",
        "      },\n",
        "      {\n",
        "        \"extension_id\": 4,\n",
        "        \"extension_text\": \"Upgrade your gaming experience with our latest collection. Buy now!\",\n",
        "        \"category\":\"callout extension\"\n",
        "      },\n",
        "      {\n",
        "        \"extension_id\": 5,\n",
        "        \"extension_text\": \"Check out the prices of our products. Buy now!\",\n",
        "        \"category\":\"Price extension\"\n",
        "      },\n",
        "      {\n",
        "        \"extension_id\": 6,\n",
        "        \"extension_text\": \"check out our app. download now!\",\n",
        "        \"category\":\"App extension\"\n",
        "      },\n",
        "      {\n",
        "        \"extension_id\": 7,\n",
        "        \"extension_text\": \"Explore all our services\",\n",
        "        \"category\":\"service extensions\"\n",
        "      }\n",
        "    ]\n",
        "  with open(json_file_path, 'w') as file:\n",
        "      json.dump(json_data, file, indent=4)\n",
        "\n",
        "        # Define the path to the JSON file\n",
        "  json_file = \"keyword_extraction_output.json\"\n",
        "\n",
        "  # Load the JSON data\n",
        "  with open(json_file) as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "  # Print the loaded data\n",
        "  #print(data)\n",
        "\n",
        "  # Read data from JSON file\n",
        "  with open('keyword_extraction_output.json', 'r') as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "  # Extract ad extensions and keywords from the data\n",
        "  ad_extensions = data['ad_extensions']\n",
        "  keywords = data['Top Keywords per Document']\n",
        "\n",
        "  # Prepare the data in the desired format for training the model\n",
        "  X = keywords  # Input features (keywords)\n",
        "  y = ad_extensions  # Output labels (ad extensions)\n",
        "\n",
        "\n",
        "  # Further processing or splitting into training/testing sets can be done as needed\n",
        "\n",
        "    # Read data from JSON file\n",
        "  with open('keyword_extraction_output.json', 'r') as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "  # Extract keywords from the data\n",
        "  keywords = data['Top Keywords per Document']\n",
        "\n",
        "  # Convert the keywords to string format\n",
        "  keywords = [' '.join(keyword) for keyword in keywords]\n",
        "\n",
        "  # Create a CountVectorizer to convert keywords into a bag-of-words representation\n",
        "  vectorizer = CountVectorizer()\n",
        "\n",
        "  # Fit and transform the keywords to obtain the bag-of-words representation\n",
        "  features = vectorizer.fit_transform(keywords)\n",
        "\n",
        "  # Convert the bag-of-words representation to a numerical feature matrix\n",
        "  feature_matrix = features.toarray()\n",
        "\n",
        "\n",
        "    # Read data from JSON file\n",
        "  with open('keyword_extraction_output.json', 'r') as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "  # Add the feature matrix to the data\n",
        "  data['Feature Matrix'] = feature_matrix.tolist()\n",
        "\n",
        "  # Save the updated data to the JSON file\n",
        "  with open('keyword_extraction_output.json', 'w') as file:\n",
        "      json.dump(data, file, indent=4)\n",
        "\n",
        "    # Define the path to the JSON file\n",
        "  json_file = \"keyword_extraction_output.json\"\n",
        "\n",
        "  # Load the JSON data\n",
        "  with open(json_file) as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "\n",
        "\n",
        "    # Read the ad_extensions_data.json file\n",
        "  with open('keyword_extraction_output.json', 'r') as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "  # Extract the ad_extensions from the data\n",
        "  ad_extensions = data[\"ad_extensions\"]\n",
        "\n",
        "  # Extract the category labels from the ad_extensions\n",
        "  categories = [extension[\"category\"] for extension in ad_extensions]\n",
        "\n",
        "  # Perform label encoding\n",
        "  label_encoder = LabelEncoder()\n",
        "  encoded_labels = label_encoder.fit_transform(categories)\n",
        "\n",
        "  # Update the ad_extensions with the encoded labels\n",
        "  for i, extension in enumerate(ad_extensions):\n",
        "      extension[\"category_encoded\"] = int(encoded_labels[i])\n",
        "\n",
        "  # Save the updated ad_extensions_data.json file to a new file\n",
        "  output_file = 'ad_extensions_data_encoded.json'\n",
        "  with open(output_file, 'w') as file:\n",
        "      json.dump(data, file, indent=4)\n",
        "\n",
        "    # Define the path to the JSON file\n",
        "  json_file = \"ad_extensions_data_encoded.json\"\n",
        "\n",
        "  # Load the JSON data\n",
        "  with open(json_file) as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "\n",
        "    # Load the data\n",
        "  with open('ad_extensions_data_encoded.json', 'r') as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "  # Extract the features and labels\n",
        "  keywords = data[\"Feature Matrix\"]\n",
        "  ad_extensions = data[\"ad_extensions\"]\n",
        "\n",
        "  # Check the number of samples in each data\n",
        "  num_samples_keywords = len(keywords)\n",
        "  num_samples_ad_extensions = len(ad_extensions)\n",
        "\n",
        "  # Determine the minimum number of samples\n",
        "  min_num_samples = min(num_samples_keywords, num_samples_ad_extensions)\n",
        "\n",
        "  # Trim the data to have the same number of samples\n",
        "  keywords = keywords[:min_num_samples]\n",
        "  ad_extensions = ad_extensions[:min_num_samples]\n",
        "\n",
        "  # Extract the labels\n",
        "  labels = [extension[\"category_encoded\"] for extension in ad_extensions]\n",
        "\n",
        "  # Split the data into training and testing sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(keywords, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Convert the lists to numpy arrays\n",
        "  X_train = np.array(X_train)\n",
        "  X_test = np.array(X_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "    # Load the data\n",
        "  with open('ad_extensions_data_encoded.json', 'r') as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "  # Extract the features and labels\n",
        "  keywords = data[\"Feature Matrix\"]\n",
        "  ad_extensions = data[\"ad_extensions\"]\n",
        "\n",
        "  # Check the number of samples in each dataset\n",
        "  num_samples_keywords = len(keywords)\n",
        "  num_samples_ad_extensions = len(ad_extensions)\n",
        "\n",
        "  # Determine the minimum number of samples\n",
        "  min_num_samples = min(num_samples_keywords, num_samples_ad_extensions)\n",
        "\n",
        "  # Trim the data to have the same number of samples\n",
        "  keywords = keywords[:min_num_samples]\n",
        "  ad_extensions = ad_extensions[:min_num_samples]\n",
        "\n",
        "  # Extract the labels\n",
        "  labels = [extension[\"category_encoded\"] for extension in ad_extensions]\n",
        "\n",
        "  # Split the data into training and testing sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(keywords, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Convert the lists to numpy arrays\n",
        "  X_train = np.array(X_train)\n",
        "  X_test = np.array(X_test)\n",
        "  y_train = np.array(y_train)\n",
        "  y_test = np.array(y_test)\n",
        "\n",
        "  # Define the random forest classifier\n",
        "  rf = RandomForestClassifier()\n",
        "\n",
        "  # Define the parameter grid for hyperparameter tuning\n",
        "  param_grid = {\n",
        "      'n_estimators': [50, 100, 150],\n",
        "      'max_depth': [None, 5, 10],\n",
        "      'min_samples_split': [2, 5, 10]\n",
        "  }\n",
        "\n",
        "  # Create GridSearchCV object with LeaveOneOut\n",
        "  grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=LeaveOneOut(), scoring='accuracy')\n",
        "\n",
        "  # Perform grid search cross-validation\n",
        "  grid_search.fit(X_train, y_train)\n",
        "\n",
        "  # Get the best model from grid search\n",
        "  best_model = grid_search.best_estimator_\n",
        "\n",
        "  # Evaluate the best model on the test set\n",
        "  test_accuracy = best_model.score(X_test, y_test)\n",
        "  #print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import LSTM, Dense\n",
        "\n",
        "  # Reshape the input arrays\n",
        "  X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "  X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "\n",
        "  # Define the model architecture\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "  model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  # Train the model\n",
        "  model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "  from keras.models import load_model\n",
        "\n",
        "  model.save('my_model.h5')\n",
        "  model = load_model('my_model.h5')\n",
        "\n",
        "    # Reshape the test data\n",
        "  num_samples = X_test.shape[0]\n",
        "  num_timesteps = X_test.shape[1]\n",
        "  num_features = X_test.shape[2]\n",
        "  X_test_reshaped = X_test.reshape(num_samples, num_timesteps, num_features)\n",
        "\n",
        "\n",
        "  # Use the trained model to make predictions on the test data\n",
        "  predictions = model.predict(X_test_reshaped)\n",
        "\n",
        "  # Get the predicted labels\n",
        "  predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "  # Decode the predicted labels using the label_encoder\n",
        "  predicted_categories = label_encoder.inverse_transform(predicted_labels)\n",
        "\n",
        "  return predicted_categories\n",
        "\n",
        "  # # Print the predicted categories and corresponding true categories\n",
        "  # print(\"Predicted Categories\")\n",
        "  # for i in range(len(predicted_categories)):\n",
        "  #     print(predicted_categories[i])\n"
      ],
      "metadata": {
        "id": "mMmzpyL_cHEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url=input(\"Enter the Url\").strip()\n",
        "x=main(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFvHhCROvnpA",
        "outputId": "a2871fae-e253-46bc-e50d-e02fb2cc9e5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the Urlhttps://www.vjshearingfolding.in/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 1.9564 - accuracy: 0.0000e+00 - val_loss: 1.9467 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 1.9500 - accuracy: 0.0000e+00 - val_loss: 1.9499 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.9436 - accuracy: 0.0000e+00 - val_loss: 1.9532 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.9372 - accuracy: 0.2000 - val_loss: 1.9565 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.9308 - accuracy: 0.2000 - val_loss: 1.9598 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.9245 - accuracy: 0.2000 - val_loss: 1.9631 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.9181 - accuracy: 0.6000 - val_loss: 1.9664 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.9118 - accuracy: 0.6000 - val_loss: 1.9697 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.9054 - accuracy: 0.6000 - val_loss: 1.9730 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.8991 - accuracy: 0.8000 - val_loss: 1.9764 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 414ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLgPCgKmZIK6",
        "outputId": "5b6a12ff-6d68-4b9d-81f4-988fcf2d1119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['service extensions' 'App extension']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}